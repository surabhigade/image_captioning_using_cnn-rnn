{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f30d91d",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "This notebook implements and trains a CNN-RNN model with attention for image captioning using 10% of the Flickr8k dataset for initial testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a9dc4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppress TensorFlow warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00de9f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/surabhigade/VSCodeProjects/cnn_rnn/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Install TensorFlow if not already installed\n",
    "# %pip install tensorflow\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, Add\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7d2d4c",
   "metadata": {},
   "source": [
    "## Load and Sample Preprocessed Data\n",
    "Load the preprocessed data and sample 10% for initial training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8984c2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original training samples: 6000\n",
      "Sampled training samples: 600\n",
      "Original validation samples: 1000\n",
      "Sampled validation samples: 100\n",
      "\n",
      "Data sampling and preprocessing completed successfully\n"
     ]
    }
   ],
   "source": [
    "# Load preprocessed data\n",
    "processed_dir = '../data/processed/'\n",
    "\n",
    "with open(os.path.join(processed_dir, 'train_features.pkl'), 'rb') as f:\n",
    "    train_features = pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(processed_dir, 'train_captions.pkl'), 'rb') as f:\n",
    "    train_captions = pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(processed_dir, 'val_features.pkl'), 'rb') as f:\n",
    "    val_features = pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(processed_dir, 'val_captions.pkl'), 'rb') as f:\n",
    "    val_captions = pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(processed_dir, 'tokenizer.pkl'), 'rb') as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "\n",
    "# Calculate vocabulary size and maximum sequence length\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "# Calculate max length from training captions\n",
    "max_length = max(len(caption.split()) for captions_list in train_captions.values() \n",
    "                for caption in captions_list)\n",
    "\n",
    "print(f'Vocabulary size: {vocab_size}')\n",
    "print(f'Maximum sequence length: {max_length}')\n",
    "\n",
    "# Sample 10% of training data instead of 1%\n",
    "np.random.seed(42)  # for reproducibility\n",
    "train_ids = list(train_features.keys())\n",
    "sampled_train_ids = np.random.choice(train_ids, size=int(len(train_ids) * 0.1), replace=False)\n",
    "\n",
    "# Sample 10% of validation data\n",
    "val_ids = list(val_features.keys())\n",
    "sampled_val_ids = np.random.choice(val_ids, size=int(len(val_ids) * 0.1), replace=False)\n",
    "\n",
    "# Create sampled datasets\n",
    "sampled_train_features = {k: train_features[k] for k in sampled_train_ids}\n",
    "sampled_train_captions = {k: train_captions[k] for k in sampled_train_ids}\n",
    "sampled_val_features = {k: val_features[k] for k in sampled_val_ids}\n",
    "sampled_val_captions = {k: val_captions[k] for k in sampled_val_ids}\n",
    "\n",
    "print(f'Original training samples: {len(train_features)}')\n",
    "print(f'Sampled training samples: {len(sampled_train_features)}')\n",
    "print(f'Original validation samples: {len(val_features)}')\n",
    "print(f'Sampled validation samples: {len(sampled_val_features)}')\n",
    "\n",
    "# Replace original data with sampled data\n",
    "train_features = sampled_train_features\n",
    "train_captions = sampled_train_captions\n",
    "val_features = sampled_val_features\n",
    "val_captions = sampled_val_captions\n",
    "\n",
    "# Preprocess image features to match model input shape\n",
    "def preprocess_image_features(features):\n",
    "    processed_features = {}\n",
    "    for img_id, feature in features.items():\n",
    "        if feature.shape != (224, 224, 3):\n",
    "            # Reshape if necessary\n",
    "            feature = tf.image.resize(feature, (224, 224))\n",
    "        processed_features[img_id] = feature\n",
    "    return processed_features\n",
    "\n",
    "train_features = preprocess_image_features(train_features)\n",
    "val_features = preprocess_image_features(val_features)\n",
    "\n",
    "print('\\nData sampling and preprocessing completed successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9057d7fc",
   "metadata": {},
   "source": [
    "## Define the CNN-RNN Model with Attention\n",
    "Create the model architecture combining CNN features with RNN and attention mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e092e99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
    "        # hidden shape == (batch_size, hidden_size)\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
    "\n",
    "        # score shape == (batch_size, 64, 1)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(features) + self.W2(hidden_with_time_axis)))\n",
    "\n",
    "        # attention_weights shape == (batch_size, 64, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, embedding_dim)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54d6c7cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 40\u001b[0m\n\u001b[1;32m     34\u001b[0m features_shape \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m3\u001b[39m)  \u001b[38;5;66;03m# VGG16 input shape\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Create and compile model\u001b[39;00m\n\u001b[1;32m     37\u001b[0m model \u001b[38;5;241m=\u001b[39m create_model(\n\u001b[1;32m     38\u001b[0m     embedding_dim\u001b[38;5;241m=\u001b[39membedding_dim,\n\u001b[1;32m     39\u001b[0m     units\u001b[38;5;241m=\u001b[39munits,\n\u001b[0;32m---> 40\u001b[0m     vocab_size\u001b[38;5;241m=\u001b[39m\u001b[43mvocab_size\u001b[49m,\n\u001b[1;32m     41\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[1;32m     42\u001b[0m     features_shape\u001b[38;5;241m=\u001b[39mfeatures_shape\n\u001b[1;32m     43\u001b[0m )\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Compile with appropriate loss and optimizer\u001b[39;00m\n\u001b[1;32m     46\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m     47\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m),\n\u001b[1;32m     48\u001b[0m     loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     49\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     50\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vocab_size' is not defined"
     ]
    }
   ],
   "source": [
    "def create_model(embedding_dim, units, vocab_size, max_length, features_shape):\n",
    "    # Image encoder\n",
    "    inputs1 = Input(shape=features_shape)\n",
    "    fe1 = Dropout(0.4)(inputs1)\n",
    "    fe2 = Dense(embedding_dim, activation='relu')(fe1)\n",
    "\n",
    "    # Sequence encoder\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    se1 = Embedding(vocab_size, embedding_dim, mask_zero=True)(inputs2)\n",
    "    se2 = Dropout(0.4)(se1)\n",
    "\n",
    "    # Decoder with attention\n",
    "    decoder1 = LSTM(units, return_sequences=True)(se2)\n",
    "    decoder2 = LSTM(units)(decoder1)\n",
    "\n",
    "    # Attention mechanism\n",
    "    attention = BahdanauAttention(units)\n",
    "    context_vector, attention_weights = attention(fe2, decoder2)\n",
    "\n",
    "    # Combine context vector with decoder output\n",
    "    decoder3 = tf.concat([context_vector, decoder2], axis=-1)\n",
    "\n",
    "    # Dense layers\n",
    "    outputs = Dense(512, activation='relu')(decoder3)\n",
    "    outputs = Dropout(0.4)(outputs)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(outputs)\n",
    "\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# Model parameters\n",
    "embedding_dim = 256\n",
    "units = 512\n",
    "features_shape = (224, 224, 3)  # VGG16 input shape\n",
    "\n",
    "# Create and compile model\n",
    "model = create_model(\n",
    "    embedding_dim=embedding_dim,\n",
    "    units=units,\n",
    "    vocab_size=vocab_size,\n",
    "    max_length=max_length,\n",
    "    features_shape=features_shape\n",
    ")\n",
    "\n",
    "# Compile with appropriate loss and optimizer\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7396be",
   "metadata": {},
   "source": [
    "## Data Generator\n",
    "Create a generator to feed data to the model during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a1b33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(features, captions, tokenizer, max_length, vocab_size, batch_size):\n",
    "    # Get image IDs and their corresponding captions\n",
    "    all_image_ids = list(captions.keys())\n",
    "    \n",
    "    while True:\n",
    "        # Shuffle image IDs at the start of each epoch\n",
    "        np.random.shuffle(all_image_ids)\n",
    "        \n",
    "        for i in range(0, len(all_image_ids), batch_size):\n",
    "            batch_image_ids = all_image_ids[i:i + batch_size]\n",
    "            \n",
    "            # Initialize batch arrays\n",
    "            X1 = []  # Images\n",
    "            X2 = []  # Input sequences\n",
    "            y = []   # Output words\n",
    "            \n",
    "            # Process each image in the batch\n",
    "            for image_id in batch_image_ids:\n",
    "                image = features[image_id]\n",
    "                captions_list = captions[image_id]\n",
    "                \n",
    "                # Randomly select one caption for the image\n",
    "                caption = np.random.choice(captions_list)\n",
    "                \n",
    "                # Convert caption to sequence\n",
    "                seq = tokenizer.texts_to_sequences([caption])[0]\n",
    "                \n",
    "                # Generate input-output pairs\n",
    "                for i in range(1, len(seq)):\n",
    "                    in_seq = seq[:i]\n",
    "                    out_seq = seq[i]\n",
    "                    \n",
    "                    # Pad input sequence\n",
    "                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "                    \n",
    "                    # One-hot encode output word\n",
    "                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "                    \n",
    "                    # Add to batch\n",
    "                    X1.append(image)\n",
    "                    X2.append(in_seq)\n",
    "                    y.append(out_seq)\n",
    "            \n",
    "            if len(X1) > 0:  # Only yield if we have data\n",
    "                yield ([np.array(X1), np.array(X2)], np.array(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811d2c60",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "Set up training parameters and train the model with callbacks for checkpointing and early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc66347b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters - adjusted batch size\n",
    "batch_size = 32  # Reduced from 64 to handle smaller dataset better\n",
    "epochs = 20  # Increased epochs since we have more data now\n",
    "\n",
    "# Calculate steps per epoch - revised calculation\n",
    "train_descriptions = sum([[desc for desc in captions] for captions in train_captions.values()], [])\n",
    "val_descriptions = sum([[desc for desc in captions] for captions in val_captions.values()], [])\n",
    "\n",
    "steps_per_epoch = len(train_descriptions) // batch_size\n",
    "validation_steps = len(val_descriptions) // batch_size\n",
    "\n",
    "print(f'Number of training descriptions: {len(train_descriptions)}')\n",
    "print(f'Number of validation descriptions: {len(val_descriptions)}')\n",
    "print(f'Steps per epoch: {steps_per_epoch}')\n",
    "print(f'Validation steps: {validation_steps}')\n",
    "\n",
    "# Create model checkpoint callback\n",
    "checkpoint_path = '../models/model_checkpoint.h5'\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    checkpoint_path,\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "# Create early stopping callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Create training and validation generators\n",
    "train_generator = data_generator(\n",
    "    train_features,\n",
    "    train_captions,\n",
    "    tokenizer,\n",
    "    max_length,\n",
    "    vocab_size,\n",
    "    batch_size\n",
    ")\n",
    "\n",
    "val_generator = data_generator(\n",
    "    val_features,\n",
    "    val_captions,\n",
    "    tokenizer,\n",
    "    max_length,\n",
    "    vocab_size,\n",
    "    batch_size\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "try:\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        epochs=epochs,\n",
    "        validation_data=val_generator,\n",
    "        validation_steps=validation_steps,\n",
    "        callbacks=[checkpoint, early_stopping],\n",
    "        verbose=1\n",
    "    )\n",
    "    print('Training completed successfully')\n",
    "except Exception as e:\n",
    "    print(f'Error during training: {str(e)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c54966b",
   "metadata": {},
   "source": [
    "## Save the Model\n",
    "Save the trained model and training history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15a0253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "model.save('../models/cnn_rnn_attention.h5')\n",
    "print('Model saved successfully.')\n",
    "\n",
    "# Save training history\n",
    "with open('../models/training_history.pkl', 'wb') as f:\n",
    "    pickle.dump(history.history, f)\n",
    "print('Training history saved successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16486a49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202aed90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290c523b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e755c64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
